{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "124cc7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "class TfidfVectorizer_manual:\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        vocabulary = set()\n",
    "        if isinstance(dataset, (list,)):\n",
    "            for data_point in dataset:\n",
    "                words_list = data_point.split(\" \")\n",
    "                for each_word in words_list:\n",
    "                    vocabulary.add(each_word)\n",
    "        vocabulary = list(vocabulary)\n",
    "        vocabulary.sort()\n",
    "        idf_values = self.idf_values(dataset, vocabulary)\n",
    "        return vocabulary, idf_values\n",
    "\n",
    "    def transform(self, dataset, vocabulary, idf_vocabulary):\n",
    "        vocabulary_dict = {k: v for v, k in enumerate(vocabulary)}\n",
    "        col = []\n",
    "        row = []\n",
    "        values = []\n",
    "        for row_index in range(len(dataset)):\n",
    "            values_row = []\n",
    "            data_point_unique = list(set(dataset[row_index].split(\" \")))\n",
    "            data_points = dataset[row_index].split(\" \")\n",
    "            for each_feature in data_point_unique:\n",
    "                col.append(vocabulary_dict[each_feature])\n",
    "                row.append(row_index)\n",
    "                idf = idf_vocabulary[vocabulary_dict[each_feature]]\n",
    "                values_row.append(idf * (data_points.count(each_feature) / len(dataset[row_index].split(\" \"))))\n",
    "            values_row_array = np.array(values_row).reshape(1, -1)\n",
    "            values_normalized = preprocessing.normalize(values_row_array, norm='l2').tolist()\n",
    "            values.extend(values_normalized[0])\n",
    "        return csr_matrix((values, (row, col)), shape=(len(dataset), len(vocabulary)))\n",
    "\n",
    "    def idf_values(self, dataset, sorted_vocabulary):\n",
    "        idf_values = []\n",
    "        no_of_document = len(dataset)\n",
    "        for each_feature in sorted_vocabulary:\n",
    "            each_feature_occurance = self.get_occurance(each_feature, dataset)\n",
    "            idf_values.append(1 + math.log((no_of_document + 1) / (each_feature_occurance + 1)))\n",
    "        return idf_values\n",
    "\n",
    "    def get_occurance(self, feature, dataset):\n",
    "        count = 0\n",
    "        for each_datapoint in dataset:\n",
    "            features_set = set(each_datapoint.split(\" \"))\n",
    "            if feature in features_set:\n",
    "                count += 1\n",
    "        return count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "349d5c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'this is the first document',\n",
    "    'this document is the second document',\n",
    "    'and this is the third one',\n",
    "    'is this the first document',\n",
    "]\n",
    "\n",
    "tfidf = TfidfVectorizer_manual()\n",
    "vocab, idf_values = tfidf.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0e6a1fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "[1.916290731874155, 1.2231435513142097, 1.5108256237659907, 1.0, 1.916290731874155, 1.916290731874155, 1.0, 1.916290731874155, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(vocab)\n",
    "print(idf_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55145904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.4697913855799205\n",
      "  (0, 2)\t0.580285823684436\n",
      "  (0, 3)\t0.3840852409148149\n",
      "  (0, 6)\t0.3840852409148149\n",
      "  (0, 8)\t0.3840852409148149\n",
      "  (1, 1)\t0.6876235979836937\n",
      "  (1, 3)\t0.2810886740337529\n",
      "  (1, 5)\t0.5386476208856762\n",
      "  (1, 6)\t0.2810886740337529\n",
      "  (1, 8)\t0.2810886740337529\n",
      "  (2, 0)\t0.511848512707169\n",
      "  (2, 3)\t0.267103787642168\n",
      "  (2, 4)\t0.511848512707169\n",
      "  (2, 6)\t0.267103787642168\n",
      "  (2, 7)\t0.511848512707169\n",
      "  (2, 8)\t0.267103787642168\n",
      "  (3, 1)\t0.4697913855799205\n",
      "  (3, 2)\t0.580285823684436\n",
      "  (3, 3)\t0.3840852409148149\n",
      "  (3, 6)\t0.3840852409148149\n",
      "  (3, 8)\t0.3840852409148149\n"
     ]
    }
   ],
   "source": [
    "csr_matrix = tfidf.transform(corpus, vocab, idf_values)\n",
    "print(csr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95155b3a",
   "metadata": {},
   "source": [
    "### Sklearn Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15656b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.91629073, 1.22314355, 1.51082562, 1.        , 1.91629073,\n",
       "       1.91629073, 1.        , 1.91629073, 1.        ])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "vectorizer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a234f64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "csr_matrix = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9b88497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t0.38408524091481483\n",
      "  (0, 6)\t0.38408524091481483\n",
      "  (0, 3)\t0.38408524091481483\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 1)\t0.46979138557992045\n",
      "  (1, 8)\t0.281088674033753\n",
      "  (1, 6)\t0.281088674033753\n",
      "  (1, 5)\t0.5386476208856763\n",
      "  (1, 3)\t0.281088674033753\n",
      "  (1, 1)\t0.6876235979836938\n",
      "  (2, 8)\t0.267103787642168\n",
      "  (2, 7)\t0.511848512707169\n",
      "  (2, 6)\t0.267103787642168\n",
      "  (2, 4)\t0.511848512707169\n",
      "  (2, 3)\t0.267103787642168\n",
      "  (2, 0)\t0.511848512707169\n",
      "  (3, 8)\t0.38408524091481483\n",
      "  (3, 6)\t0.38408524091481483\n",
      "  (3, 3)\t0.38408524091481483\n",
      "  (3, 2)\t0.5802858236844359\n",
      "  (3, 1)\t0.46979138557992045\n"
     ]
    }
   ],
   "source": [
    "print(csr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ed1260",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "771d49af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from itertools import islice\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class TfidfVectorizer_limit:\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        vocabulary = set()\n",
    "        if isinstance(dataset, (list,)):\n",
    "            for data_point in dataset:\n",
    "                words_list = data_point.split(\" \")\n",
    "                for each_word in words_list:\n",
    "                    vocabulary.add(each_word)\n",
    "        vocabulary = list(vocabulary)\n",
    "        vocabulary.sort()\n",
    "        idf_values = self.idf_values(dataset, vocabulary)\n",
    "        sorted_vocab_idf = {k: v for k, v in sorted(idf_values.items(), key=lambda item: item[1], reverse=True)}\n",
    "        sorted_vocab_idf_50_elements = dict(islice(sorted_vocab_idf.items(), 50))\n",
    "        vocabulary = sorted_vocab_idf_50_elements.keys()\n",
    "        idf_values = list(sorted_vocab_idf_50_elements.values())\n",
    "        return vocabulary, idf_values\n",
    "\n",
    "    def idf_values(self, dataset, vocabulary):\n",
    "        idf_values_vocab = {}\n",
    "        no_of_document = len(dataset)\n",
    "        for index in range(len(vocabulary)):\n",
    "            each_feature_occurance = self.get_occurance(vocabulary[index], dataset)\n",
    "            idf_values_vocab[vocabulary[index]] = (1 + math.log((no_of_document + 1) / (each_feature_occurance + 1)))\n",
    "        return idf_values_vocab\n",
    "\n",
    "    def get_occurance(self, feature, dataset):\n",
    "        count = 0\n",
    "        for each_datapoint in dataset:\n",
    "            features_set = set(each_datapoint.split(\" \"))\n",
    "            if feature in features_set:\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "    def transform(self, dataset, vocabulary, idf_vocabulary):\n",
    "        vocabulary_dict = {k: v for v, k in enumerate(vocabulary)}\n",
    "        col = []\n",
    "        row = []\n",
    "        values = []\n",
    "        for row_index in range(len(dataset)):\n",
    "            values_row = []\n",
    "            data_point_split_list = list(set(dataset[row_index].split(\" \")))\n",
    "            for each_feature in data_point_split_list:\n",
    "                if each_feature in vocabulary:\n",
    "                    col.append(vocabulary_dict[each_feature])\n",
    "                    row.append(row_index)\n",
    "                    idf = idf_vocabulary[vocabulary_dict[each_feature]]\n",
    "                    values_row.append(idf * (data_point_split_list.count(each_feature) / len(data_point_split_list)))\n",
    "            if len(values_row) > 0:\n",
    "                values_row_array = np.array(values_row).reshape(1, -1)\n",
    "                values_normalized = preprocessing.normalize(values_row_array, norm='l2').tolist()\n",
    "                values.extend(values_normalized[0])\n",
    "        return csr_matrix((values, (row, col)), shape=(len(dataset), len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35d1e97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/Users/kunalbudhiraja/Desktop/Code/AppliedAI/cleaned_string.csv\", header=None).to_numpy()\n",
    "corpus_global = []\n",
    "\n",
    "for each in data:\n",
    "    corpus_global.append(each[0])\n",
    "\n",
    "tfidf = TfidfVectorizer_limit()\n",
    "vocab, idf = tfidf.fit(corpus_global)\n",
    "sparse_matrix = tfidf.transform(corpus_global, vocab, idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "317df754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['aailiyah', 'abandoned', 'abroad', 'abstruse', 'academy', 'accents', 'accessible', 'acclaimed', 'accolades', 'accurate', 'accurately', 'achille', 'ackerman', 'actions', 'adams', 'add', 'added', 'admins', 'admiration', 'admitted', 'adrift', 'adventure', 'aesthetically', 'affected', 'affleck', 'afternoon', 'aged', 'ages', 'agree', 'agreed', 'aimless', 'aired', 'akasha', 'akin', 'alert', 'alike', 'allison', 'allow', 'allowing', 'alongside', 'amateurish', 'amaze', 'amazed', 'amazingly', 'amusing', 'amust', 'anatomist', 'angel', 'angela', 'angelina'])\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "91b4eeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872]\n"
     ]
    }
   ],
   "source": [
    "print(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6960a627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 30)\t1.0\n",
      "  (68, 24)\t1.0\n",
      "  (72, 29)\t1.0\n",
      "  (74, 31)\t1.0\n",
      "  (119, 33)\t1.0\n",
      "  (135, 3)\t0.3779644730092272\n",
      "  (135, 10)\t0.3779644730092272\n",
      "  (135, 18)\t0.3779644730092272\n",
      "  (135, 20)\t0.3779644730092272\n",
      "  (135, 36)\t0.3779644730092272\n",
      "  (135, 40)\t0.3779644730092272\n",
      "  (135, 41)\t0.3779644730092272\n",
      "  (176, 49)\t1.0\n",
      "  (181, 13)\t1.0\n",
      "  (192, 21)\t1.0\n",
      "  (193, 23)\t1.0\n",
      "  (216, 2)\t1.0\n",
      "  (222, 47)\t1.0\n",
      "  (225, 19)\t1.0\n",
      "  (227, 17)\t1.0\n",
      "  (241, 44)\t1.0\n",
      "  (270, 1)\t1.0\n",
      "  (290, 25)\t1.0\n",
      "  (333, 26)\t1.0\n",
      "  (334, 15)\t1.0\n",
      "  (341, 43)\t1.0\n",
      "  (344, 42)\t1.0\n",
      "  (348, 8)\t1.0\n",
      "  (377, 37)\t1.0\n",
      "  (409, 5)\t1.0\n",
      "  (430, 39)\t1.0\n",
      "  (457, 45)\t1.0\n",
      "  (461, 4)\t1.0\n",
      "  (465, 38)\t1.0\n",
      "  (475, 35)\t1.0\n",
      "  (493, 6)\t1.0\n",
      "  (500, 48)\t1.0\n",
      "  (548, 0)\t0.7071067811865475\n",
      "  (548, 32)\t0.7071067811865475\n",
      "  (608, 14)\t1.0\n",
      "  (612, 11)\t1.0\n",
      "  (620, 46)\t1.0\n",
      "  (632, 7)\t1.0\n",
      "  (644, 12)\t0.7071067811865475\n",
      "  (644, 27)\t0.7071067811865475\n",
      "  (664, 28)\t1.0\n",
      "  (667, 22)\t1.0\n",
      "  (691, 34)\t1.0\n",
      "  (697, 9)\t1.0\n",
      "  (722, 16)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04428a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_36_env",
   "language": "python",
   "name": "py_36_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
